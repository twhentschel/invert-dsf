---
jupytext:
  formats: ipynb,md:myst
  text_representation:
    extension: .md
    format_name: myst
    format_version: 0.13
    jupytext_version: 1.16.0
kernelspec:
  display_name: Python 3 (ipykernel)
  language: python
  name: python3
---

# Bayesian Inference with Markov Chain Monte Carlo: Ideal Dataset

This notebook is roughly identical with `5.0-twh-....ipynb` except that the ELF data for this notebook is generated from our collision model _fit to Average Atom collision rate data_. In other words, the collision rate we are trying to find is exactly representable by our model, so we are testing the limits of the inversion process.

```{code-cell} ipython3
# Load the "autoreload" extension so that code can change
%load_ext autoreload

# always reload modules so that as you change code in src, it gets loaded
%autoreload 2
```

```{code-cell} ipython3
import numpy as np
rng = np.random.default_rng()
import matplotlib.pyplot as plt
from scipy import optimize
from functools import partial

import emcee 
from multiprocessing import Pool

from uegdielectric import ElectronGas
from uegdielectric.dielectric import Mermin

from src.inference.collision_models import collision_activate_decay
from src.inference.mcmc_inference import unique_hdf5_group
from src.utilities import collrateimag, AtomicUnits, elec_loss_fn

import warnings
warnings.filterwarnings('ignore')
```

# Get the data

```{code-cell} ipython3
datafile = "../../data/external/Al-1 _vwf.txt"
# just copying from the data file
# temperature
teV = 1
t = teV / AtomicUnits.energy

# density
d_ang = 0.18071 # 1/[angstroms]**3
d = d_ang * AtomicUnits.length**3

datadump = np.loadtxt(datafile, skiprows=9, usecols=[0, 5, 6, 9], unpack=True)
orig_freq_data = datadump[0] # [eV]
print(f"length of original data = {len(orig_freq_data)}")
AA_collrate_data = datadump[1] + 1j * datadump[2]
dos_data = datadump[3]

# function for Average Atom collision rate
AA_collrate_fn = lambda x : np.interp(x, orig_freq_data / AtomicUnits.energy, AA_collrate_data)
# function for DOS ratio
dos_fn = lambda x : np.interp(x, orig_freq_data / AtomicUnits.energy, dos_data)

# create a new, more regular frequency grid that what exists in the data file
freq_data = np.geomspace(orig_freq_data[0], orig_freq_data[-1], 99) # [eV]
# Wavenumber is independent of data -- can pick whatever we want to create our ELF data
wavenum = 1.5 # 1/[angstrom]
```

# Define collision rate model

```{code-cell} ipython3
# define our collision rate function
def collisionrate(freq, params):
    # real part
    recollision = collision_activate_decay(freq, *params)
    # imaginary part
    imcollision = collrateimag(recollision)

    return recollision + 1j * imcollision
```

## Fit our collision rate model to AA collision rate

Note that we are only fitting to the real part of the data.
The imaginary part is found from Kramers-Kronig transformation.

```{code-cell} ipython3
# fit to data
popt_AAcoll, pcov = optimize.curve_fit(
    collision_activate_decay, 
    freq_data / AtomicUnits.energy,
    AA_collrate_fn(freq_data / AtomicUnits.energy).real,
    (1,1,1,1,1),
    bounds=(0, np.inf)
)
print(popt)

# plot to compare
plt.semilogx(freq_data, (AAcoll := AA_collrate_fn(freq_data / AtomicUnits.energy)).real, c="C0", label="AA")
plt.plot(freq_data, AAcoll.imag, c="C0", ls="--")
plt.plot(freq_data, (modelcoll := collisionrate(freq_data / AtomicUnits.energy, popt_AAcoll)).real, c="C3", label="opt")
plt.plot(freq_data, modelcoll.imag, c="C3", ls="--")
plt.xlabel(r"$\hbar\omega$ [eV]")
plt.ylabel("collision rate [au]")
plt.legend()
```

### Create ELF data with our collision rate  model using parameters fit to mimic the average atom collision rate data

```{code-cell} ipython3
# true ELF data
elf_data = elec_loss_fn(Mermin(ElectronGas(t, d, dos_fn)),
                        wavenum * AtomicUnits.length,
                        freq_data / AtomicUnits.energy,
                        lambda x : collisionrate(x, popt_AAcoll)
                    )
print(f"shape of ELF data = {elf_data.shape}")
```

# Define residual function for ELF data

```{code-cell} ipython3
def residual_fn(
    freq, 
    ydata,
    dielectricfn, 
    wavenum, 
    collisionfn, 
    params, 
    type="abs"
):
    """
    Returns a 1D residual function between the data and the normalized ELF model.
    Can be used with multi-dimensional array inputs as long as `ydata`
    and `dielectricfn(wavenum, freq, lambda x: collisionfn(x, params))`
    are broadcastable (if `len(wavenum) == m` and `len(freq) == n`,
    then `dielectricfn(wavenum, freq, lambda x: collisionfn(x, params))`
    has a shape (m, n)).

    freq: frequency data points corresponding to output data `ydata`, atomic units
    ydata: ELF data points, atomic units
    dielectricfn: dielectric function, a function of `wavenum` and `x`
    wavenum: wave number value(s), atomic units
    collisionfn: function for collision rate, depends on `freq` and the
        parameters `params` we want to optimize
    params (tuple): parameters that go into collision rate model `collisionfn`
    type (string): the type of residual function. There are four choices:
        "abs" - absolute residual: ydata - model
        "rel" - relative residual: (ydata - model) / ydata

        The different types will tend to emphasize different features in the
        data that the parameters of the model should be optimized to fit.
    
    returns:
        residual of the model with respect to the data `ydata`. If `ydata`
        has shape (m, n), then return array has shape (m * n, ).
    """
    # normalized ELF calculations
    elf_calc = elec_loss_fn(dielectricfn,
                            wavenum,
                            freq,
                            lambda x: collisionfn(x, params)
                           )
    
    match type:
        case "abs":
            weight = 1
        case "rel":
            weight = ydata
        case _:
            raise ValueError(f"residual type {type} not accepted")
            
    return ((ydata - elf_calc) / weight).flatten()
```

```{code-cell} ipython3
# setup
residual = partial(residual_fn,
                   freq_data / AtomicUnits.energy,
                   elf_data,
                   Mermin(ElectronGas(t, d, dos_fn)),
                   wavenum * AtomicUnits.length,
                   collisionrate,
                   type="rel"
                  )
# initial parameter guesses
initparams = (1, 1, 1, 1, 1)

# optimization results
optresult = optimize.least_squares(residual, initparams, bounds=(0, np.inf), max_nfev=150)
```

```{code-cell} ipython3
optresult
```

```{code-cell} ipython3
# original
plt.semilogx(freq_data, collisionrate(freq_data / AtomicUnits.energy, popt_AAcoll).real, c="C3", label="true")
# optimized
plt.plot(freq_data, collisionrate(freq_data / AtomicUnits.energy, optresult.x).real, c="C0", label="opt.", ls="--")
plt.xlabel(r"$\hbar\omega$ [eV]")
plt.ylabel("collision rate [au]")
plt.legend()
```

```{code-cell} ipython3
plt.loglog(freq_data, elf_data, c="C3", label="true")
opt_elf = elec_loss_fn(Mermin(ElectronGas(t, d, dos_fn)),
                       wavenum * AtomicUnits.length,
                       freq_data / AtomicUnits.energy,
                       lambda x: collisionrate(x, optresult.x)
                      )
plt.plot(freq_data, opt_elf, c="C0", ls='-.', label="opt.")
plt.ylabel("ELF [au]")
plt.xlabel(r"$\hbar \omega$ [eV]")
plt.ylim(1e-6) # resolution of data
plt.legend()
```

# Perform MCMC

+++

The collision rate model we are using has parameters that don't run over the whole real line. For example, the `*_height` keyword argument in our parameters dictionary should be strictly positive. We can define our (log) prior distribution to be a uniform distribution over specific ranges of the real line. This will help constrain our parameters within a defined range.

```{code-cell} ipython3
def logprior(params, limits):
    """
    Uniform prior for the parameters

    params: array_like
        logarithms of the parameters of collision frequency model
    limits: array_like of length 2
        limits of the uniform distribution for all parameters/dimensions
    """
    params = np.asarray(params)
    limits = np.asarray(limits)
    # check that parameters are within their respective limits
    if np.all(limits[:, 0] < params) and np.all(params < limits[:, 1]):
        return 0.0
    return -np.inf
```

For the log likelihood function, we want to enforce equal preference to all simulation outcomes that match the data to within some uncertainty, regardless of the de-
tails of the fit. To do that, we use a cutoff function that smooths the boundaries between accepted and rejected values for the parameters. For this notebook,
we are also considering noise-free data, so we use the _relative_ residual in our likelihood.

```{code-cell} ipython3
def loglikelihood(
    params, 
    x, 
    y, 
    dielectricfn, 
    wavenum, 
    collisionfn, 
    cutoff=0.01
):
    """
    log likelihood function that is the log of an N-dimensional cut-off
    function with soft (exponential) sides.

    params (array_like): parameters that go into collision rate model
    x: frequency data points corresponding to output data `y`
    y: ELF data points
    dielectricfn: dielectric function, a function of `wavenum` and `freq`
    wavenum: wave number value(s)
    freq: array of frequency values
    collisionfn: function for collision rate, depends on `freq` and `params`
    hyperparams (dict): keyword arguments for the hyperparameters of the
        likelihood function. Default 0.01 corresponds to a cutoff of roughly
        1% agreement between data and the model -- any deviation much more
        than would return a much smaller log likelihood. 
    
    returns:
        log likelihood evaluated for the given parameters at the input `x`.
    """
    if cutoff <= 0:
        raise RuntimeError("cutoff must be positive and nonzero")
        
    residual = residual_fn(
        x,
        y,
        dielectricfn,
        wavenum,
        collisionfn,
        params,
        type="rel"
    )
        
    loglik = -np.max((residual / (np.sqrt(2) * cutoff))**2)
    
    return loglik
```

For the log posterior distribution, we combine both the log prior and log likelihood functions.

```{code-cell} ipython3
def logposterior_fn(
    params,
    x, 
    y, 
    dielectricfn, 
    wavenum, 
    collisionfn, 
    priorhyperparams={}, 
    likhyperparams={}
):
    """
    log posterior pdf.

    params: the parameters for `collisionfn`
    x: frequency data points corresponding to output data `y`
    y: ELF data points
    dielectricfn: dielectric function, a function of `wavenum` and `freq`
    wavenum: wave number value(s)
    freq: array of frequency values
    collisionfn: function for collision rate, depends on `freq` and (the exponential of) `logparams`
    likhyperparams (dict): keyword arguments for the hyperparameters of the
        prior function.
    likhyperparams (dict): keyword arguments for the hyperparameters of the
        likelihood function.
    
    returns:
        log posterior function that only depends on params of collision rate
        model.
    """
    lgprior = logprior(params, **priorhyperparams)
    lglik = loglikelihood(params,
                          x, 
                          y, 
                          dielectricfn, 
                          wavenum, 
                          collisionfn, 
                          **likhyperparams
                         )

    # if either of the prior or likelihood not finite,
    # return -infinity
    if not (np.isfinite(lgprior) and np.isfinite(lglik)):
        return -np.inf
        
    return lgprior + lglik
```

Define a (log) posterior distribution that only depends on the parameters of the collision rate model

```{code-cell} ipython3
lik_cutoff = 0.05
prior_lims = [
    [0, 5],
    [0, 5],
    [0, 5],
    [0, 100],
    [0, 100]
]

def logposterior(params):
    """
    Helper function for logposterior that treats all but the `logparams` argument as global
    variables.

    Doing it this way (as opposed to a lambda function, for example) because we need to
    pickle it for multiprocessing (see emcee docs).
    """
    return logposterior_fn(params,
                           freq_data / AtomicUnits.energy,
                           elf_data,
                           Mermin(ElectronGas(t, d, dos_fn)),
                           wavenum * AtomicUnits.length,
                           collisionrate,
                           priorhyperparams = {"limits": prior_lims},
                           likhyperparams = {"cutoff": lik_cutoff}
                          )
```

```{code-cell} ipython3
logposterior(optresult.x)
```

## Run the MCMC sampler

We will use the results from the optimization to initialize the Markov chains.

```{code-cell} ipython3
import h5py
from datetime import datetime

samplesfile = "../../data/mcmc/mcmc_modeldata"
dataset = "mcmc_samples_modeldata"
runinfo = {
    "date" : datetime.today().strftime('%a %d %b %Y, %I:%M%p'),
    "input data info" : f"""Data generated using collision rate model
        fit to average atom data from file {datafile}""",
    "input data temperature [eV]" : teV,
    "input data density [1/angstrom^3]" : d_ang,
    "input data wavenumber [1/angstrom]" : wavenum,
    "collision rate model" : "collision_activate_decay",
    "likelihood function" : f"Soft (expenonential) cutoff with a cutoff value of {lik_cutoff}",
    "prior distribution function" : f"Uniform distribution with boundaries ({prior_lims})"
}

# sampler properties
ndim = 5 # number of parameters
numchains = 32
numsamples = 25_000

# # avoid overwriting datasets
# dataset = unique_hdf5_group(samplesfile, dataset)
backend = emcee.backends.HDFBackend(samplesfile, name=dataset)
backend.reset(numchains, ndim)

# randomly initialize chains within the boundaries
rng = np.random.default_rng()
initial_state = optresult.x + 1e-4 * rng.random(size=(numchains, ndim))

# perform ensemble MCMC sampling of logposterior
with Pool() as pool:
    sampler = emcee.EnsembleSampler(
        numchains, ndim, logposterior, backend=backend, pool=pool
    )
    sampler.run_mcmc(initial_state, numsamples, progress=True) # upwards of 1 1/2 hrs!!

# add information to h5py file
with h5py.File(samplesfile, "a") as f:
    dset = f[dataset]
    for k, v in runinfo.items():
        dset.attrs[k] = v
```

```{code-cell} ipython3
# # view attributes of dataset
# with h5py.File(samplesfile, "a") as f:
#     print(list(f.keys()))
#     dset = f["mcmc_samples"]
#     for attr in dset.attrs:
#         print(f"{attr} : {dset.attrs[attr]}")
```

# Read in MCMC data

```{code-cell} ipython3
backend = emcee.backends.HDFBackend(samplesfile, name=dataset)
```

```{code-cell} ipython3
tau = backend.get_autocorr_time(tol=0)
burnin = int(2 * np.max(tau))
thin = int(0.5 * np.min(tau))
flat_samples = backend.get_chain(discard=burnin, flat=True, thin=thin)

print("Mean autocorrelation time: {0:.3f} steps".format(np.mean(tau)))
print("burn-in: {0}".format(burnin))
print("thin: {0}".format(thin))
print("flat chain shape: {0}".format(flat_samples.shape))
print("Mean acceptance fraction: {0:.3f}".format(np.mean(backend.accepted / backend.iteration)))
```

# MCMC results

```{code-cell} ipython3
plt.close()
paramnames = (
    "drude_height",
    "gendrude_height",
    "gendrude_power",
    "logistic_activate",
    "logistic_gradient"
)
fig, axes = plt.subplots(ndim, figsize=(10,2.5*5), sharex=True)
samples = sampler.get_chain()
for i in range(ndim):
    ax = axes[i]
    ax.plot(samples[:,:,i], 'k', alpha=0.3)
    #ax.set_xlim(0, len(samples))
    ax.set_ylabel(f"{paramnames[i]}")
    ax.yaxis.set_label_coords(-0.1, 0.5)

axes[-1].set_xlabel("step number");
```

```{code-cell} ipython3
import corner

fig = corner.corner(flat_samples, labels=paramnames)
```

```{code-cell} ipython3
# randomly pick 100 samples from our MCMC sampling data
inds = rng.integers(len(flat_samples), size=100)

# plot collision function for different parameters from MCMC sampling
for ind in inds:
    sample = flat_samples[ind]
    plt.loglog(freq_data, 
               collisionrate(freq_data / AtomicUnits.energy, sample), 
               "C1", 
                alpha=0.1
              )

# plot AA collision frequency
plt.plot(freq_data,
         true_collrate_fn(freq_data / AtomicUnits.energy).real,
         c="k",
         label="Avg. Atom",
         lw=2,
         ls='--'
        )

plt.xlabel(r"$\hbar\omega$ [eV]")
plt.ylabel("collision rate [au]")
# plt.ylim(1e-3)
plt.legend()
# plt.savefig("../../reports/figures/mcmc_modeldata_collisionsamples")
```

```{code-cell} ipython3
eps = Mermin(ElectronGas(t, d, dos_fn))
# plot ELF for different parameters from MCMC sampling
for ind in inds:
    sample = flat_samples[ind]
    plt.semilogx(freq_data,
                 elec_loss_fn(
                     eps,
                     wavenum * AtomicUnits.length,
                     freq_data / AtomicUnits.energy,
                     lambda x: collisionrate(x, sample)
                 ), 
                 "C1", 
                 alpha=0.1)
# plot data
plt.loglog(freq_data, elf_data, c="k", label="Avg. Atom", lw=2, ls='--')

plt.ylabel("ELF [au]")
plt.xlabel(r"$\hbar\omega$ [eV]")
plt.ylim(1e-6)
plt.legend()
# plt.savefig("../../reports/figures/mcmc_modeldata_ELFsamples")
```

```{code-cell} ipython3

```
