---
jupytext:
  formats: ipynb,md:myst
  text_representation:
    extension: .md
    format_name: myst
    format_version: 0.13
    jupytext_version: 1.16.0
kernelspec:
  display_name: Python 3 (ipykernel)
  language: python
  name: python3
---

```{code-cell} ipython3
# Load the "autoreload" extension so that code can change
%load_ext autoreload

# always reload modules so that as you change code in src, it gets loaded
%autoreload 2
```

```{code-cell} ipython3
import numpy as np
import matplotlib.pyplot as plt
from functools import partial

import emcee 
from multiprocessing import Pool

from uegdielectric import ElectronGas
from uegdielectric.dielectric import Mermin

from src.inference.collision_models import collision_activate_decay
from src.inference.mcmc_inference import unique_hdf5_group
from src.utilities import collrateimag, AtomicUnits, elec_loss_fn

import warnings
warnings.filterwarnings('ignore')
```

# Define collision rate model

```{code-cell} ipython3
# define our collision rate function
def collisionrate(freq, params):
    # real part
    recollision = collision_activate_decay(freq, *params)
    # imaginary part
    imcollision = collrateimag(recollision)

    return recollision + 1j * imcollision
```

# Get the data

```{code-cell} ipython3
datafile = "../../data/external/Al-1 _vwf.txt"
# just copying from the data file
# temperature
teV = 1
t = teV / AtomicUnits.energy

# density
d_ang = 0.18071 # 1/[angstroms]**3
d = d_ang * AtomicUnits.length**3

datadump = np.loadtxt(datafile, skiprows=9, usecols=[0, 5, 6, 9], unpack=True)
freq_data = datadump[0] # [eV]
print(f"length of data = {len(freq_data)}")
collrate_data = datadump[1] + 1j * datadump[2]
dos_data = datadump[3]

# function for true collision rate
true_collrate_fn = lambda x : np.interp(x, freq_data / AtomicUnits.energy, collrate_data)
# function for DOS ratio
dos_fn = lambda x : np.interp(x, freq_data / AtomicUnits.energy, dos_data)

# Wavenumber is independent of data -- can pick whatever we want to create our ELF data
wavenum_val = 0.7 / AtomicUnits.length # 1/[angstrom]
```

### Create ELF data with collision rate data

We will also trim down the number of data points.

```{code-cell} ipython3
# Reduce the amount of data we are considering
freq_data_trim = freq_data[::8]

elf_data = elec_loss_fn(Mermin(ElectronGas(t, d, dos_fn)),
                        wavenum_val * AtomicUnits.length,
                        freq_data_trim / AtomicUnits.energy,
                        true_collrate_fn
                       )
print(elf_data.shape)
```

# Define residual function

```{code-cell} ipython3
def residual_fn(x, y, dielectricfn, wavenum, collisionfn, params):
    """ 
    x: frequency data points corresponding to output data `y`
    y: ELF data points
    dielectricfn: dielectric function, a function of `wavenum` and `freq`
    wavenum: wave number value(s)
    freq: array of frequency values
    collisionfn: function for collision rate, depends on `freq` and the
        parameters `params` we want to optimize
    params (tuple): parameters that go into collision rate model `collisionfn`
    
    returns:
        residual of the model with respect to the data `y`
    """
    elf_calc = elec_loss_fn(dielectricfn,
                            wavenum,
                            x,
                            lambda x: collisionfn(x, params))
    return y - elf_calc
```

# Perform MCMC

+++

The collision rate model we are using has parameters that don't run over the whole real line. For example, the `*_height` keyword argument in our parameters dictionary should be strictly positive. Additionally, other arguments like the location of the Drude peaks or the sigmoid function can have wildly varying magnitudes (see the data above). It's in our best interest when performing MCMC sampling to actually work with the log of each of these parameters, and transform back in post processing.

+++

Define our prior probability density function (pdf), likelihood, and posterior pdf

```{code-cell} ipython3
def logprior(logparams, limits=(-5,5)):
    """
    Uniform prior for the (log of the) parameters

    logparams: array_like
        logarithms of the parameters of collision frequency model
    limits: array_like of length 2
        limits of the uniform distribution, same for all parameters
    """
    logparams = np.asarray(logparams)
    # check that values are within limits
    if np.all(limits[0] < logparams) and np.all(logparams < limits[1]):
        return 0.0
    return -np.inf
    
def loglikelihood(logparams, x, y, dielectricfn, wavenum, collisionfn, cutoff=1):
    """
    log likelihood function that is the log of an N-dimensional cut-off
    function with soft (exponential) sides.

    logparams (array_like): parameters that go into collision rate model
    x: frequency data points corresponding to output data `y`
    y: ELF data points
    dielectricfn: dielectric function, a function of `wavenum` and `freq`
    wavenum: wave number value(s)
    freq: array of frequency values
    collisionfn: function for collision rate, depends on `freq` and `params`
    hyperparams (dict): keyword arguments for the hyperparameters of the
        likelihood function.
    
    returns:
        log likelihood evaluated for the given parameters at the input `x`.
    """
    if cutoff <= 0:
        raise RuntimeError("cutoff must be positive and nonzero")
        
    params = np.exp(logparams)
    residual = residual_fn(x,
                           y,
                           dielectricfn,
                           wavenum,
                           collisionfn,
                           params
                          )
        
    loglik = -np.max((residual / (np.sqrt(2) * cutoff))**2)
    
    return loglik

def logposterior_fn(logparams, x, y, dielectricfn, wavenum, collisionfn, priorhyperparams={}, likhyperparams={}):
    """
    log posterior pdf.

    x: frequency data points corresponding to output data `y`
    y: ELF data points
    dielectricfn: dielectric function, a function of `wavenum` and `freq`
    wavenum: wave number value(s)
    freq: array of frequency values
    collisionfn: function for collision rate, depends on `freq` and (the exponential of) `logparams`
    logparams: the logarithms of the parameters for `collisionfn`
    likhyperparams (dict): keyword arguments for the hyperparameters of the
        prior function.
    likhyperparams (dict): keyword arguments for the hyperparameters of the
        likelihood function.
    
    returns:
        log posterior function that only depends on params of collision rate
        model.
    """
    lgprior = logprior(logparams, **priorhyperparams)
    lglik = loglikelihood(logparams,
                          x, 
                          y, 
                          dielectricfn, 
                          wavenum, 
                          collisionfn, 
                          **likhyperparams
                         )
    
    if not (np.isfinite(lgprior) and np.isfinite(lglik)):
        return -np.inf
        
    return lgprior + lglik
```

```{code-cell} ipython3
lik_cutoff = 0.05
prior_lims = (-5, 3)

def logposterior(logparams):
    """
    Helper function for logposterior that treats all but the `logparams` argument as global
    variables.

    Doing it this way (as opposed to a lambda function, for example) because we need to
    pickle it for multiprocessing (see emcee docs).
    """
    return logposterior_fn(logparams,
                           freq_data_trim / AtomicUnits.energy,
                           elf_data,
                           Mermin(ElectronGas(t, d, dos_fn)),
                           wavenum_val * AtomicUnits.length,
                           collisionrate,
                           priorhyperparams = {"limits": prior_lims},
                           likhyperparams = {"cutoff": lik_cutoff}
                          )
```

## Run the MCMC sampler

We will use the results from the optimization to initialize the Markov chains.

```{code-cell} ipython3
import h5py
from datetime import datetime

samplesfile = "../../data/mcmc/mcmc_AAcollisions"
dataset = "mcmc_samples"
runinfo = {
    "date" : datetime.today().strftime('%a %d %b %Y, %I:%M%p'),
    "input data info" : f"""Data generated using Average Atom 
         collisions [T-matrix cross sections, nonideal DOS, QOZ structure
         factors, LB form, inelastic processes], from file {samplesfile}""",
    "input data temperature [eV]" : teV,
    "input data density [1/angstrom^3]" : d_ang,
    "input data wavenumber [1/angstrom]" : wavenum_val,
    "collision rate model" : "collision_activate_decay",
    "likelihood function" : f"Soft (expenonential) cutoff with a cutoff value of {lik_cutoff}",
    "prior distribution function" : f"Uniform distribution with boundaries ({prior_lims})"
}

# sampler properties
ndim = 7 # number of parameters
numchains = 14
numsamples = 100

# # avoid overwriting datasets
# dataset = unique_hdf5_group(samplesfile, dataset)
backend = emcee.backends.HDFBackend(samplesfile, name=dataset)
backend.reset(numchains, ndim)

# randomly initialize chains within the boundaries (log scale: prior_lims)
rng = np.random.default_rng()
initial_states = prior_lims[0] \
    + (prior_lims[1] - prior_lims[0]) * rng.random(size=(numchains, ndim))

# perform ensemble MCMC sampling of logposterior
with Pool() as pool:
    sampler = emcee.EnsembleSampler(
        numchains, ndim, logposterior, backend=backend, pool=pool
    )
    sampler.run_mcmc(initial_states, numsamples, progress=True)

# add information to h5py file
with h5py.File(samplesfile, "a") as f:
    dset = f[dataset]
    for k, v in runinfo.items():
        dset.attrs[k] = v
```

```{code-cell} ipython3
# # view attributes of dataset
# with h5py.File(samplesfile, "a") as f:
#     print(list(f.keys()))
#     dset = f["mcmc_samples"]
#     for attr in dset.attrs:
#         print(f"{attr} : {dset.attrs[attr]}")
```

```{code-cell} ipython3
tau = backend.get_autocorr_time(tol=0)
burnin = int(2 * np.max(tau))
thin = int(0.5 * np.min(tau))
flat_samples = backend.get_chain(discard=burnin, flat=True, thin=thin)

print("Mean autocorrelation time: {0:.3f} steps".format(np.mean(tau)))
print("burn-in: {0}".format(burnin))
print("thin: {0}".format(thin))
print("flat chain shape: {0}".format(flat_samples.shape))
print("Mean acceptance fraction: {0:.3f}".format(np.mean(backend.accepted / backend.iteration)))
```

```{code-cell} ipython3
plt.close()
paramnames = (
    "drude_center",
    "drude_height",
    "gendrude_center",
    "gendrude_height",
    "gendrude_power",
    "logistic_activate",
    "logistic_gradient"
)
fig, axes = plt.subplots(ndim, figsize=(10,2.5*5), sharex=True)
samples = sampler.get_chain()
for i in range(ndim):
    ax = axes[i]
    # transform back to normal values (working with the log of the params)
    ax.plot(np.exp(samples[:,:,i]), 'k', alpha=0.3)
    #ax.set_xlim(0, len(samples))
    ax.set_ylabel(f"{paramnames[i]}")
    ax.yaxis.set_label_coords(-0.1, 0.5)

axes[-1].set_xlabel("step number");
```
